def evaluate_model(model, dataset, metric_fn):
    """
    Evaluate the performance of the fine-tuned model on the given dataset using the specified metric function.

    Parameters:
    - model: The fine-tuned language model to evaluate.
    - dataset: The dataset to evaluate the model on.
    - metric_fn: A function that takes model outputs and ground truth values to compute a metric.

    Returns:
    - A dictionary containing the evaluation results.
    """
    results = {
        'metric_value': 0,
        'num_samples': len(dataset),
    }

    # Iterate through the dataset and evaluate the model
    for sample in dataset:
        prompt = sample['input']
        ground_truth = sample['output']
        
        # Generate output from the model
        generated_output = model.generate(prompt)
        
        # Calculate the metric
        metric_value = metric_fn(generated_output, ground_truth)
        results['metric_value'] += metric_value

    # Average the metric value over the number of samples
    results['metric_value'] /= results['num_samples']
    
    return results

def accuracy_metric(generated_output, ground_truth):
    """
    A simple accuracy metric that compares generated output to ground truth.

    Parameters:
    - generated_output: The output generated by the model.
    - ground_truth: The expected output.

    Returns:
    - 1 if the outputs match, 0 otherwise.
    """
    return 1 if generated_output == ground_truth else 0

def log_evaluation_results(results):
    """
    Log the evaluation results to a file or console.

    Parameters:
    - results: The evaluation results to log.
    """
    print("Evaluation Results:")
    print(f"Metric Value: {results['metric_value']}")
    print(f"Number of Samples: {results['num_samples']}")