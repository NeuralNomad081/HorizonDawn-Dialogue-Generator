{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing\n",
    "\n",
    "This notebook is used for collecting and preprocessing the dataset for the game content generator. It includes code for scraping data from various sources or loading existing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-collection-setup"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create directories to store data\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping (Optional)\n",
    "\n",
    "This section contains code for scraping data from websites. You can skip this if you already have the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for data collection\n",
    "url = 'https://example.com/game-data'\n",
    "\n",
    "# Function to scrape data from the website\n",
    "def scrape_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data = []\n",
    "    \n",
    "    # Example scraping logic (modify according to the actual website structure)\n",
    "    for item in soup.find_all('div', class_='item'):\n",
    "        title = item.find('h2').text\n",
    "        description = item.find('p').text\n",
    "        data.append({'title': title, 'description': description})\n",
    "    return data\n",
    "\n",
    "# Uncomment to run scraping\n",
    "# game_data = scrape_data(url)\n",
    "# df = pd.DataFrame(game_data)\n",
    "# df.to_csv('../data/raw/game_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Processing Functions\n",
    "\n",
    "These functions process JSON dialogue files into CSV format suitable for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_standard_dialogues(dialogues, source_file, scene_name=None):\n",
    "    \"\"\"Process dialogues in the standard format.\"\"\"\n",
    "    processed_dialogues = []\n",
    "    \n",
    "    for i, dialogue in enumerate(dialogues):\n",
    "        if isinstance(dialogue, dict):\n",
    "            # If dialogue is a dictionary, extract text or lines\n",
    "            if 'text' in dialogue:\n",
    "                dialogue_text = dialogue['text']\n",
    "            elif 'lines' in dialogue:\n",
    "                # Join lines if dialogue is split into lines\n",
    "                lines = dialogue['lines']\n",
    "                if isinstance(lines, list):\n",
    "                    dialogue_text = \"\\n\".join([str(line) for line in lines])\n",
    "                else:\n",
    "                    dialogue_text = str(lines)\n",
    "            else:\n",
    "                # Use the whole dialogue dict minus metadata\n",
    "                dialogue_copy = dialogue.copy()\n",
    "                exclude_keys = ['id', 'dialogueID', 'metadata']\n",
    "                for key in exclude_keys:\n",
    "                    if key in dialogue_copy:\n",
    "                        del dialogue_copy[key]\n",
    "                dialogue_text = json.dumps(dialogue_copy)\n",
    "        elif isinstance(dialogue, str):\n",
    "            # If dialogue is directly a string\n",
    "            dialogue_text = dialogue\n",
    "        else:\n",
    "            # Skip if we can't process this dialogue\n",
    "            print(f\"Warning: Skipping dialogue {i} in {source_file} due to unknown format\")\n",
    "            continue\n",
    "        \n",
    "        # Create a dialogue entry\n",
    "        dialogue_entry = {\n",
    "            'source_file': source_file,\n",
    "            'dialogue': dialogue_text,\n",
    "            'dialogueID': dialogue.get('dialogueID', f\"{source_file}_{i}\") if isinstance(dialogue, dict) else f\"{source_file}_{i}\"\n",
    "        }\n",
    "        \n",
    "        # Add scene info if available\n",
    "        if scene_name:\n",
    "            dialogue_entry['scene'] = scene_name\n",
    "            \n",
    "        # Add any other useful metadata from the dialogue\n",
    "        if isinstance(dialogue, dict):\n",
    "            for key in ['scene', 'speaker', 'character', 'context']:\n",
    "                if key in dialogue:\n",
    "                    dialogue_entry[key] = dialogue[key]\n",
    "        \n",
    "        processed_dialogues.append(dialogue_entry)\n",
    "    \n",
    "    return processed_dialogues\n",
    "\n",
    "def process_text_list_format(text_list, source_file):\n",
    "    \"\"\"Process the format found in page02.json and similar files.\"\"\"\n",
    "    processed_dialogues = []\n",
    "    \n",
    "    # Group the dialogue exchanges together\n",
    "    current_dialogue = []\n",
    "    dialogue_blocks = []\n",
    "    \n",
    "    for item in text_list:\n",
    "        if isinstance(item, dict):\n",
    "            # Each item should be a dict with a single key-value pair\n",
    "            # (speaker/action: text)\n",
    "            current_dialogue.append(item)\n",
    "            \n",
    "            # Check if this is a separator (like {\"ACTION\": \"---\"})\n",
    "            is_separator = False\n",
    "            for key, value in item.items():\n",
    "                if key == \"ACTION\" and (value == \"---\" or value.startswith(\"-----\")):\n",
    "                    is_separator = True\n",
    "                    \n",
    "            if is_separator and current_dialogue:\n",
    "                # End of a dialogue section\n",
    "                if len(current_dialogue) > 1:  # Only save if there's actual dialogue\n",
    "                    dialogue_blocks.append(current_dialogue[:-1])  # Exclude separator\n",
    "                current_dialogue = [item]  # Keep separator as start of next block\n",
    "    \n",
    "    # Don't forget the last dialogue if it doesn't end with a separator\n",
    "    if current_dialogue:\n",
    "        dialogue_blocks.append(current_dialogue)\n",
    "    \n",
    "    # Process each dialogue block\n",
    "    for i, block in enumerate(dialogue_blocks):\n",
    "        dialogue_lines = []\n",
    "        \n",
    "        for item in block:\n",
    "            for speaker, text in item.items():\n",
    "                if speaker == \"ACTION\":\n",
    "                    dialogue_lines.append(f\"[{text}]\")\n",
    "                else:\n",
    "                    dialogue_lines.append(f\"{speaker}: {text}\")\n",
    "        \n",
    "        dialogue_text = \"\\n\".join(dialogue_lines)\n",
    "        \n",
    "        dialogue_entry = {\n",
    "            'source_file': source_file,\n",
    "            'dialogue': dialogue_text,\n",
    "            'dialogueID': f\"{source_file}_block_{i}\"\n",
    "        }\n",
    "        \n",
    "        processed_dialogues.append(dialogue_entry)\n",
    "    \n",
    "    return processed_dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Function\n",
    "\n",
    "This function processes all JSON files into CSV format ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-preprocessing"
   },
   "outputs": [],
   "source": [
    "def process_json_to_csv(raw_dir=\"../data/raw\", processed_dir=\"../data/processed\"):\n",
    "    \"\"\"Process JSON files in raw_dir to CSV format for training dialogue generation models.\n",
    "    \n",
    "    The script creates:\n",
    "    - raw1_dialogues.csv: All dialogues from all sources\n",
    "    - raw1_train.csv: Training split (80% of data)\n",
    "    - raw1_val.csv: Validation split (20% of data)\n",
    "    - train_formatted.csv, val_formatted.csv: Same splits but with improved formatting\n",
    "    \"\"\"\n",
    "    print(f\"Processing JSON files from {raw_dir} to {processed_dir}\")\n",
    "    \n",
    "    # Ensure the processed directory exists\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all JSON files\n",
    "    json_files = [f for f in os.listdir(raw_dir) if f.endswith('.json')]\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {raw_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "    \n",
    "    # Collect all dialogues\n",
    "    all_dialogues = []\n",
    "    \n",
    "    for file in json_files:\n",
    "        source_file = file\n",
    "        file_path = os.path.join(raw_dir, file)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract dialogues based on the structure of your JSON files\n",
    "            if isinstance(data, dict) and 'dialogues' in data:\n",
    "                # Format: {\"dialogues\": [...]}\n",
    "                dialogues = data['dialogues']\n",
    "                processed_dialogues = process_standard_dialogues(dialogues, source_file)\n",
    "                all_dialogues.extend(processed_dialogues)\n",
    "                \n",
    "            elif isinstance(data, dict) and 'scenes' in data:\n",
    "                # Format: {\"scenes\": [...]} where each scene has dialogues\n",
    "                processed_dialogues = []\n",
    "                for scene in data['scenes']:\n",
    "                    if 'dialogues' in scene:\n",
    "                        scene_dialogues = process_standard_dialogues(scene['dialogues'], source_file, scene.get('name', 'Unknown Scene'))\n",
    "                        processed_dialogues.extend(scene_dialogues)\n",
    "                all_dialogues.extend(processed_dialogues)\n",
    "                \n",
    "            elif isinstance(data, list):\n",
    "                # List of dialogues directly\n",
    "                processed_dialogues = process_standard_dialogues(data, source_file)\n",
    "                all_dialogues.extend(processed_dialogues)\n",
    "                \n",
    "            elif isinstance(data, dict) and 'text' in data and isinstance(data['text'], list):\n",
    "                # Format from page02.json: {\"text\": [{speaker/action: content}, ...]}\n",
    "                processed_dialogues = process_text_list_format(data['text'], source_file)\n",
    "                all_dialogues.extend(processed_dialogues)\n",
    "                \n",
    "            else:\n",
    "                # Try processing any other dict format at the top level\n",
    "                processed = False\n",
    "                \n",
    "                # Check for any list field that might contain dialogues\n",
    "                for key, value in data.items():\n",
    "                    if isinstance(value, list) and len(value) > 0:\n",
    "                        if all(isinstance(item, dict) for item in value):\n",
    "                            print(f\"Found list field '{key}' in {file}, attempting to process...\")\n",
    "                            processed_dialogues = process_text_list_format(value, source_file)\n",
    "                            if processed_dialogues:\n",
    "                                all_dialogues.extend(processed_dialogues)\n",
    "                                processed = True\n",
    "                                break\n",
    "                \n",
    "                if not processed:\n",
    "                    # Last resort: treat the whole file as a single dialogue entry\n",
    "                    dialogue_text = json.dumps(data)\n",
    "                    all_dialogues.append({\n",
    "                        'source_file': source_file,\n",
    "                        'dialogue': dialogue_text,\n",
    "                        'dialogueID': f\"{source_file}_whole\"\n",
    "                    })\n",
    "                    print(f\"Processed {file} as a single dialogue entry (fallback method)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "    \n",
    "    if not all_dialogues:\n",
    "        print(\"No dialogues were extracted from the JSON files\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Extracted {len(all_dialogues)} dialogues from all files\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    dialogues_df = pd.DataFrame(all_dialogues)\n",
    "    \n",
    "    # Save all dialogues\n",
    "    dialogues_path = os.path.join(processed_dir, \"raw1_dialogues.csv\")\n",
    "    dialogues_df.to_csv(dialogues_path, index=False)\n",
    "    print(f\"Saved all dialogues to {dialogues_path}\")\n",
    "    \n",
    "    # Create training examples with prompts\n",
    "    examples = []\n",
    "    for _, row in dialogues_df.iterrows():\n",
    "        source = row['source_file']\n",
    "        if isinstance(source, str) and source.endswith('.json'):\n",
    "            source = source.replace('.json', '')\n",
    "            \n",
    "        dialogue = row['dialogue']\n",
    "        if isinstance(dialogue, str) and dialogue.strip():\n",
    "            prompt = f\"Generate dialogue for scene '{source}':\"\n",
    "            example_dict = {\n",
    "                'source': source,\n",
    "                'prompt': prompt,\n",
    "                'completion': dialogue,\n",
    "                'full_text': f\"{prompt}\\n\\n{dialogue}\"\n",
    "            }\n",
    "            \n",
    "            # Add dialogueID\n",
    "            example_dict['dialogueID'] = row['dialogueID']\n",
    "            \n",
    "            # Add any other metadata columns\n",
    "            for key in ['scene', 'speaker', 'character', 'context']:\n",
    "                if key in row and not pd.isna(row[key]):\n",
    "                    example_dict[key] = row[key]\n",
    "                    \n",
    "            examples.append(example_dict)\n",
    "    \n",
    "    # Convert to DataFrame and split\n",
    "    examples_df = pd.DataFrame(examples)\n",
    "    \n",
    "    if examples_df.empty:\n",
    "        print(\"No valid examples could be created from the dialogues\")\n",
    "        return\n",
    "    \n",
    "    # Shuffle data\n",
    "    examples_df = examples_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split into train/val\n",
    "    train_df, val_df = train_test_split(examples_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Save train/val splits\n",
    "    train_path = os.path.join(processed_dir, \"raw1_train.csv\")\n",
    "    val_path = os.path.join(processed_dir, \"raw1_val.csv\")\n",
    "    \n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    val_df.to_csv(val_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(train_df)} training examples to {train_path}\")\n",
    "    print(f\"Saved {len(val_df)} validation examples to {val_path}\")\n",
    "    \n",
    "    # Create formatted versions (you can add additional formatting if needed)\n",
    "    train_formatted_path = os.path.join(processed_dir, \"train_formatted.csv\")\n",
    "    val_formatted_path = os.path.join(processed_dir, \"val_formatted.csv\")\n",
    "    \n",
    "    train_df.to_csv(train_formatted_path, index=False)\n",
    "    val_df.to_csv(val_formatted_path, index=False)\n",
    "    \n",
    "    print(f\"Saved formatted training data to {train_formatted_path}\")\n",
    "    print(f\"Saved formatted validation data to {val_formatted_path}\")\n",
    "    \n",
    "    print(\"\\nData processing complete!\")\n",
    "    print(f\"CSV files are ready at {processed_dir}\")\n",
    "    \n",
    "    # Display a sample for verification\n",
    "    print(\"\\nSample data (first row from training set):\")\n",
    "    for col in ['prompt', 'completion', 'full_text']:\n",
    "        if col in train_df.columns:\n",
    "            print(f\"\\n{col}:\")\n",
    "            sample_text = train_df[col].iloc[0]\n",
    "            print(sample_text[:150] + \"...\" if len(sample_text) > 150 else sample_text)\n",
    "    \n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Processing Pipeline\n",
    "\n",
    "Execute this cell to process all JSON files into CSV format ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the JSON files into CSV format\n",
    "train_df, val_df = process_json_to_csv(\n",
    "    raw_dir=\"../data/raw\",  # Path to raw JSON files\n",
    "    processed_dir=\"../data/processed\"  # Path to save processed CSV files\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Explore the processed data to better understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the training data\n",
    "if 'train_df' in locals() and not train_df.empty:\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(\"\\nColumn information:\")\n",
    "    print(train_df.info())\n",
    "    print(\"\\nSample data:\")\n",
    "    display(train_df.head())\n",
    "else:\n",
    "    # Load from file if not already in memory\n",
    "    try:\n",
    "        train_df = pd.read_csv(\"../data/processed/train_formatted.csv\")\n",
    "        print(f\"Loaded training data from file. Shape: {train_df.shape}\")\n",
    "        display(train_df.head())\n",
    "    except FileNotFoundError:\n",
    "        print(\"Training data file not found. Run the processing pipeline first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
